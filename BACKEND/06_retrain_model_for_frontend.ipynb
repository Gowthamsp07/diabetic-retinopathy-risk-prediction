{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Model for Frontend Integration\n",
    "\n",
    "This notebook retrains the model to properly match the frontend input pipeline.\n",
    "It ensures feature encoding matches exactly what the backend receives from the frontend.\n",
    "\n",
    "## âš ï¸ IMPORTANT: If you see \"ModuleNotFoundError\"\n",
    "Run the cell below first to install required packages, OR change the kernel to use the virtual environment Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing required packages...\n",
      "  âœ… pandas already installed\n",
      "  âœ… numpy already installed\n",
      "  ðŸ“¥ Installing scikit-learn...\n",
      "  âœ… scikit-learn installed\n",
      "  âœ… joblib already installed\n",
      "\n",
      "âœ… All packages ready! You can now run the next cell.\n"
     ]
    }
   ],
   "source": [
    "# QUICK FIX: Install missing packages\n",
    "# Run this cell if you get \"ModuleNotFoundError: No module named 'pandas'\"\n",
    "# This will install all required packages in the current Python environment\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = ['pandas', 'numpy', 'scikit-learn', 'joblib']\n",
    "\n",
    "print(\"ðŸ“¦ Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"  âœ… {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"  ðŸ“¥ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"  âœ… {package} installed\")\n",
    "\n",
    "print(\"\\nâœ… All packages ready! You can now run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# INSTALL PACKAGES IF NOT AVAILABLE (Run this if you get ModuleNotFoundError)\n",
    "# Uncomment the line below and run this cell first if packages are missing\n",
    "# !pip install pandas numpy scikit-learn joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check Python version and working directory\n",
    "print(f\"âœ… Python version: {sys.version}\")\n",
    "print(f\"âœ… Python executable: {sys.executable}\")\n",
    "print(f\"âœ… Working directory: {os.getcwd()}\")\n",
    "print(f\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path(\"../data/diabetic_data.csv\")\n",
    "\n",
    "# Check if file exists\n",
    "if not data_path.exists():\n",
    "    # Try alternative paths\n",
    "    alt_paths = [\n",
    "        Path(\"data/diabetic_data.csv\"),\n",
    "        Path(\"../BACKEND/data/diabetic_data.csv\"),\n",
    "        Path(\"../../BACKEND/data/diabetic_data.csv\")\n",
    "    ]\n",
    "    for alt_path in alt_paths:\n",
    "        if alt_path.exists():\n",
    "            data_path = alt_path\n",
    "            print(f\"ðŸ“ Found data at: {data_path}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find diabetic_data.csv. Checked: {data_path} and alternatives\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"âœ… Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst few columns: {list(df.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "df.replace(\"?\", np.nan, inplace=True)\n",
    "df.drop(columns=[\"encounter_id\", \"patient_nbr\"], inplace=True, errors='ignore')\n",
    "\n",
    "# Convert age ranges to numeric\n",
    "age_map = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25,\n",
    "    '[30-40)': 35, '[40-50)': 45, '[50-60)': 55,\n",
    "    '[60-70)': 65, '[70-80)': 75, '[80-90)': 85,\n",
    "    '[90-100)': 95\n",
    "}\n",
    "df[\"age\"] = df[\"age\"].map(age_map)\n",
    "\n",
    "print(\"âœ… Data cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable (high_risk for diabetic retinopathy)\n",
    "df[\"high_risk\"] = np.where(\n",
    "    (df[\"A1Cresult\"].isin([\">8\", \">7\"])) |\n",
    "    (df[\"max_glu_serum\"] == \">300\") |\n",
    "    (df[\"number_diagnoses\"] >= 7),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "print(f\"âœ… Target created\")\n",
    "print(f\"High risk (1): {df['high_risk'].sum()} ({df['high_risk'].mean()*100:.1f}%)\")\n",
    "print(f\"Low risk (0): {(df['high_risk']==0).sum()} ({(1-df['high_risk'].mean())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features that match backend API\n",
    "features = [\n",
    "    \"age\", \"gender\", \"time_in_hospital\",\n",
    "    \"num_lab_procedures\", \"num_medications\",\n",
    "    \"number_outpatient\", \"number_emergency\",\n",
    "    \"number_inpatient\", \"number_diagnoses\",\n",
    "    \"insulin\", \"diabetesMed\"\n",
    "]\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[\"high_risk\"].copy()\n",
    "\n",
    "print(f\"âœ… Features selected: {len(features)} features\")\n",
    "print(f\"\\nUnique values in categorical columns:\")\n",
    "print(f\"gender: {X['gender'].unique()}\")\n",
    "print(f\"insulin: {X['insulin'].unique()}\")\n",
    "print(f\"diabetesMed: {X['diabetesMed'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and normalize categorical values to match frontend\n",
    "X = X.fillna({\n",
    "    'gender': 'Male',\n",
    "    'insulin': 'No',\n",
    "    'diabetesMed': 'No'\n",
    "})\n",
    "\n",
    "# Normalize gender (frontend sends \"Male\" or \"Female\")\n",
    "X['gender'] = X['gender'].map({\n",
    "    'Male': 'Male',\n",
    "    'Female': 'Female',\n",
    "    'Unknown/Invalid': 'Male',\n",
    "}).fillna('Male')\n",
    "\n",
    "# Normalize insulin (frontend sends \"Yes\" or \"No\")\n",
    "X['insulin'] = X['insulin'].map({\n",
    "    'Up': 'Yes',\n",
    "    'Down': 'Yes',\n",
    "    'Steady': 'Yes',\n",
    "    'No': 'No'\n",
    "}).fillna('No')\n",
    "\n",
    "# Normalize diabetesMed (frontend sends \"Yes\" or \"No\")\n",
    "X['diabetesMed'] = X['diabetesMed'].map({\n",
    "    'Yes': 'Yes',\n",
    "    'No': 'No',\n",
    "    'Ch': 'Yes'  # Change = Yes\n",
    "}).fillna('No')\n",
    "\n",
    "print(\"âœ… Categorical values normalized\")\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"gender: {X['gender'].unique()}\")\n",
    "print(f\"insulin: {X['insulin'].unique()}\")\n",
    "print(f\"diabetesMed: {X['diabetesMed'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables (drop_first=True to avoid multicollinearity)\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(f\"âœ… One-hot encoding complete\")\n",
    "print(f\"Features after encoding: {X_encoded.shape[1]} columns\")\n",
    "print(f\"\\nFeature names:\")\n",
    "for i, col in enumerate(X_encoded.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data split complete\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining target distribution:\")\n",
    "print(f\"  High risk: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "print(f\"  Low risk: {(y_train==0).sum()} ({(1-y_train.mean())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Features scaled\")\n",
    "print(f\"Scaled training shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with optimized hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training model...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"âœ… Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"ðŸ“ˆ ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"-\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"-\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Low    High\")\n",
    "print(f\"Actual Low   {cm[0,0]:5d}  {cm[0,1]:5d}\")\n",
    "print(f\"       High  {cm[1,0]:5d}  {cm[1,1]:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "model_dir = Path(\"../model\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = model_dir / \"ann_model.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"âœ… Model saved to: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = model_dir / \"scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ… Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save feature names (CRITICAL for prediction)\n",
    "feature_names_path = model_dir / \"feature_names.pkl\"\n",
    "joblib.dump(list(X_encoded.columns), feature_names_path)\n",
    "print(f\"âœ… Feature names saved to: {feature_names_path}\")\n",
    "print(f\"\\nðŸ“‹ Saved {len(X_encoded.columns)} feature names:\")\n",
    "for i, name in enumerate(list(X_encoded.columns), 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction with sample data matching frontend format\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING PREDICTION PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample data matching what frontend sends\n",
    "test_sample = {\n",
    "    'age': 50,\n",
    "    'gender': 'Male',\n",
    "    'time_in_hospital': 5,\n",
    "    'num_lab_procedures': 45,\n",
    "    'num_medications': 15,\n",
    "    'number_outpatient': 2,\n",
    "    'number_emergency': 1,\n",
    "    'number_inpatient': 1,\n",
    "    'number_diagnoses': 5,\n",
    "    'insulin': 'Yes',\n",
    "    'diabetesMed': 'Yes'\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“¥ Input data:\")\n",
    "for key, value in test_sample.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame([test_sample])\n",
    "\n",
    "# One-hot encode (same as training)\n",
    "test_encoded = pd.get_dummies(test_df, drop_first=True)\n",
    "\n",
    "# Align with training features (fill missing with 0)\n",
    "test_aligned = test_encoded.reindex(columns=X_encoded.columns, fill_value=0)\n",
    "\n",
    "print(f\"\\nâœ… Encoded features: {test_aligned.shape[1]} columns\")\n",
    "print(f\"\\nðŸ“Š Feature values (non-zero):\")\n",
    "non_zero = test_aligned.loc[0, test_aligned.loc[0] != 0]\n",
    "for col, val in non_zero.items():\n",
    "    print(f\"  {col}: {val}\")\n",
    "\n",
    "# Scale\n",
    "test_scaled = scaler.transform(test_aligned)\n",
    "\n",
    "# Predict\n",
    "probability = model.predict_proba(test_scaled)[0][1]\n",
    "risk_level = model.predict(test_scaled)[0]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Prediction Results:\")\n",
    "print(f\"  Risk Probability: {probability*100:.2f}%\")\n",
    "print(f\"  Risk Level: {'High Risk' if risk_level == 1 else 'Low Risk'}\")\n",
    "print(f\"\\nâœ… Prediction pipeline working correctly!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
