{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Model for Frontend Integration\n",
    "\n",
    "This notebook retrains the model to properly match the frontend input pipeline.\n",
    "It ensures feature encoding matches exactly what the backend receives from the frontend.\n",
    "\n",
    "## ðŸ“‹ What This Notebook Does\n",
    "\n",
    "1. **Loads and cleans** the training data\n",
    "2. **Normalizes categorical values** to match frontend format (Male/Female, Yes/No)\n",
    "3. **Trains a new model** with optimized hyperparameters\n",
    "4. **Evaluates** the model performance\n",
    "5. **Automatically backs up** old model files (if they exist)\n",
    "6. **Saves new model** artifacts to `../model/` directory\n",
    "7. **Tests** the prediction pipeline\n",
    "\n",
    "## âš ï¸ IMPORTANT Notes\n",
    "\n",
    "- **Old Model:** The notebook will automatically backup existing model files before saving new ones\n",
    "- **No Manual Removal Needed:** You don't need to delete old model files - they'll be backed up automatically\n",
    "- **After Training:** Restart your backend server to load the new model\n",
    "- **If you see \"ModuleNotFoundError\":** Run the \"QUICK FIX\" cell below first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing required packages...\n",
      "  âœ… pandas already installed\n",
      "  âœ… numpy already installed\n",
      "  ðŸ“¥ Installing scikit-learn...\n",
      "  âœ… scikit-learn installed\n",
      "  âœ… joblib already installed\n",
      "\n",
      "âœ… All packages ready! You can now run the next cell.\n"
     ]
    }
   ],
   "source": [
    "# QUICK FIX: Install missing packages\n",
    "# Run this cell if you get \"ModuleNotFoundError: No module named 'pandas'\"\n",
    "# This will install all required packages in the current Python environment\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = ['pandas', 'numpy', 'scikit-learn', 'joblib']\n",
    "\n",
    "print(\"ðŸ“¦ Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"  âœ… {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"  ðŸ“¥ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        print(f\"  âœ… {package} installed\")\n",
    "\n",
    "print(\"\\nâœ… All packages ready! You can now run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Python version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "âœ… Python executable: C:\\Users\\ASUS\\OneDrive\\Desktop\\DR risk predictor\\DR risk predictor\\BACKEND\\venv\\Scripts\\python.exe\n",
      "âœ… Working directory: C:\\Users\\ASUS\\OneDrive\\Desktop\\DR risk predictor\\DR risk predictor\\BACKEND\\notebooks\n",
      "âœ… Imports complete\n"
     ]
    }
   ],
   "source": [
    "# INSTALL PACKAGES IF NOT AVAILABLE (Run this if you get ModuleNotFoundError)\n",
    "# Uncomment the line below and run this cell first if packages are missing\n",
    "# !pip install pandas numpy scikit-learn joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check Python version and working directory\n",
    "print(f\"âœ… Python version: {sys.version}\")\n",
    "print(f\"âœ… Python executable: {sys.executable}\")\n",
    "print(f\"âœ… Working directory: {os.getcwd()}\")\n",
    "print(f\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded: 101766 rows, 50 columns\n",
      "\n",
      "First few columns: ['encounter_id', 'patient_nbr', 'race', 'gender', 'age', 'weight', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = Path(\"../data/diabetic_data.csv\")\n",
    "\n",
    "# Check if file exists\n",
    "if not data_path.exists():\n",
    "    # Try alternative paths\n",
    "    alt_paths = [\n",
    "        Path(\"data/diabetic_data.csv\"),\n",
    "        Path(\"../BACKEND/data/diabetic_data.csv\"),\n",
    "        Path(\"../../BACKEND/data/diabetic_data.csv\")\n",
    "    ]\n",
    "    for alt_path in alt_paths:\n",
    "        if alt_path.exists():\n",
    "            data_path = alt_path\n",
    "            print(f\"ðŸ“ Found data at: {data_path}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find diabetic_data.csv. Checked: {data_path} and alternatives\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"âœ… Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst few columns: {list(df.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data cleaned\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "df.replace(\"?\", np.nan, inplace=True)\n",
    "df.drop(columns=[\"encounter_id\", \"patient_nbr\"], inplace=True, errors='ignore')\n",
    "\n",
    "# Convert age ranges to numeric\n",
    "age_map = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25,\n",
    "    '[30-40)': 35, '[40-50)': 45, '[50-60)': 55,\n",
    "    '[60-70)': 65, '[70-80)': 75, '[80-90)': 85,\n",
    "    '[90-100)': 95\n",
    "}\n",
    "df[\"age\"] = df[\"age\"].map(age_map)\n",
    "\n",
    "print(\"âœ… Data cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Target created\n",
      "High risk (1): 75058 (73.8%)\n",
      "Low risk (0): 26708 (26.2%)\n"
     ]
    }
   ],
   "source": [
    "# Create target variable (high_risk for diabetic retinopathy)\n",
    "df[\"high_risk\"] = np.where(\n",
    "    (df[\"A1Cresult\"].isin([\">8\", \">7\"])) |\n",
    "    (df[\"max_glu_serum\"] == \">300\") |\n",
    "    (df[\"number_diagnoses\"] >= 7),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "print(f\"âœ… Target created\")\n",
    "print(f\"High risk (1): {df['high_risk'].sum()} ({df['high_risk'].mean()*100:.1f}%)\")\n",
    "print(f\"Low risk (0): {(df['high_risk']==0).sum()} ({(1-df['high_risk'].mean())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features selected: 11 features\n",
      "\n",
      "Unique values in categorical columns:\n",
      "gender: ['Female' 'Male' 'Unknown/Invalid']\n",
      "insulin: ['No' 'Up' 'Steady' 'Down']\n",
      "diabetesMed: ['No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Select features that match backend API\n",
    "features = [\n",
    "    \"age\", \"gender\", \"time_in_hospital\",\n",
    "    \"num_lab_procedures\", \"num_medications\",\n",
    "    \"number_outpatient\", \"number_emergency\",\n",
    "    \"number_inpatient\", \"number_diagnoses\",\n",
    "    \"insulin\", \"diabetesMed\"\n",
    "]\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[\"high_risk\"].copy()\n",
    "\n",
    "print(f\"âœ… Features selected: {len(features)} features\")\n",
    "print(f\"\\nUnique values in categorical columns:\")\n",
    "print(f\"gender: {X['gender'].unique()}\")\n",
    "print(f\"insulin: {X['insulin'].unique()}\")\n",
    "print(f\"diabetesMed: {X['diabetesMed'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Categorical values normalized\n",
      "\n",
      "After normalization:\n",
      "gender: ['Female' 'Male']\n",
      "insulin: ['No' 'Yes']\n",
      "diabetesMed: ['No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values and normalize categorical values to match frontend\n",
    "X = X.fillna({\n",
    "    'gender': 'Male',\n",
    "    'insulin': 'No',\n",
    "    'diabetesMed': 'No'\n",
    "})\n",
    "\n",
    "# Normalize gender (frontend sends \"Male\" or \"Female\")\n",
    "X['gender'] = X['gender'].map({\n",
    "    'Male': 'Male',\n",
    "    'Female': 'Female',\n",
    "    'Unknown/Invalid': 'Male',\n",
    "}).fillna('Male')\n",
    "\n",
    "# Normalize insulin (frontend sends \"Yes\" or \"No\")\n",
    "X['insulin'] = X['insulin'].map({\n",
    "    'Up': 'Yes',\n",
    "    'Down': 'Yes',\n",
    "    'Steady': 'Yes',\n",
    "    'No': 'No'\n",
    "}).fillna('No')\n",
    "\n",
    "# Normalize diabetesMed (frontend sends \"Yes\" or \"No\")\n",
    "X['diabetesMed'] = X['diabetesMed'].map({\n",
    "    'Yes': 'Yes',\n",
    "    'No': 'No',\n",
    "    'Ch': 'Yes'  # Change = Yes\n",
    "}).fillna('No')\n",
    "\n",
    "print(\"âœ… Categorical values normalized\")\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"gender: {X['gender'].unique()}\")\n",
    "print(f\"insulin: {X['insulin'].unique()}\")\n",
    "print(f\"diabetesMed: {X['diabetesMed'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… One-hot encoding complete\n",
      "Features after encoding: 11 columns\n",
      "\n",
      "Feature names:\n",
      " 1. age\n",
      " 2. time_in_hospital\n",
      " 3. num_lab_procedures\n",
      " 4. num_medications\n",
      " 5. number_outpatient\n",
      " 6. number_emergency\n",
      " 7. number_inpatient\n",
      " 8. number_diagnoses\n",
      " 9. gender_Male\n",
      "10. insulin_Yes\n",
      "11. diabetesMed_Yes\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical variables (drop_first=True to avoid multicollinearity)\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(f\"âœ… One-hot encoding complete\")\n",
    "print(f\"Features after encoding: {X_encoded.shape[1]} columns\")\n",
    "print(f\"\\nFeature names:\")\n",
    "for i, col in enumerate(X_encoded.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data split complete\n",
      "Training set: 81412 samples\n",
      "Test set: 20354 samples\n",
      "\n",
      "Training target distribution:\n",
      "  High risk: 60046 (73.8%)\n",
      "  Low risk: 21366 (26.2%)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data split complete\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining target distribution:\")\n",
    "print(f\"  High risk: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "print(f\"  Low risk: {(y_train==0).sum()} ({(1-y_train.mean())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features scaled\n",
      "Scaled training shape: (81412, 11)\n",
      "Scaled test shape: (20354, 11)\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Features scaled\")\n",
    "print(f\"Scaled training shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training model...\n",
      "Iteration 1, loss = 0.16028900\n",
      "Validation score: 0.956399\n",
      "Iteration 2, loss = 0.11087367\n",
      "Validation score: 0.956276\n",
      "Iteration 3, loss = 0.10911361\n",
      "Validation score: 0.956522\n",
      "Iteration 4, loss = 0.10843965\n",
      "Validation score: 0.955048\n",
      "Iteration 5, loss = 0.10882660\n",
      "Validation score: 0.956767\n",
      "Iteration 6, loss = 0.10779000\n",
      "Validation score: 0.956153\n",
      "Iteration 7, loss = 0.10786731\n",
      "Validation score: 0.956153\n",
      "Iteration 8, loss = 0.10775629\n",
      "Validation score: 0.955539\n",
      "Iteration 9, loss = 0.10701019\n",
      "Validation score: 0.956153\n",
      "Iteration 10, loss = 0.10677156\n",
      "Validation score: 0.957013\n",
      "Iteration 11, loss = 0.10707989\n",
      "Validation score: 0.957013\n",
      "Iteration 12, loss = 0.10677449\n",
      "Validation score: 0.956276\n",
      "Iteration 13, loss = 0.10627220\n",
      "Validation score: 0.955171\n",
      "Iteration 14, loss = 0.10640486\n",
      "Validation score: 0.955416\n",
      "Iteration 15, loss = 0.10640272\n",
      "Validation score: 0.955785\n",
      "Iteration 16, loss = 0.10594462\n",
      "Validation score: 0.954311\n",
      "Iteration 17, loss = 0.10580429\n",
      "Validation score: 0.955416\n",
      "Iteration 18, loss = 0.10542087\n",
      "Validation score: 0.956276\n",
      "Iteration 19, loss = 0.10546615\n",
      "Validation score: 0.956276\n",
      "Iteration 20, loss = 0.10502234\n",
      "Validation score: 0.956153\n",
      "Iteration 21, loss = 0.10472345\n",
      "Validation score: 0.956645\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "âœ… Model training complete\n"
     ]
    }
   ],
   "source": [
    "# Train model with optimized hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training model...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"âœ… Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Accuracy: 95.67%\n",
      "ðŸ“ˆ ROC-AUC Score: 0.9852\n",
      "\n",
      "------------------------------------------------------------\n",
      "Classification Report:\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Low Risk       0.86      0.99      0.92      5342\n",
      "   High Risk       1.00      0.94      0.97     15012\n",
      "\n",
      "    accuracy                           0.96     20354\n",
      "   macro avg       0.93      0.97      0.95     20354\n",
      "weighted avg       0.96      0.96      0.96     20354\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Confusion Matrix:\n",
      "------------------------------------------------------------\n",
      "                Predicted\n",
      "              Low    High\n",
      "Actual Low    5292     50\n",
      "       High    832  14180\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"ðŸ“ˆ ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"-\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"-\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Low    High\")\n",
    "print(f\"Actual Low   {cm[0,0]:5d}  {cm[0,1]:5d}\")\n",
    "print(f\"       High  {cm[1,0]:5d}  {cm[1,1]:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to: ..\\model\\ann_model.pkl\n",
      "âœ… Scaler saved to: ..\\model\\scaler.pkl\n",
      "âœ… Feature names saved to: ..\\model\\feature_names.pkl\n",
      "\n",
      "ðŸ“‹ Saved 11 feature names:\n",
      "   1. age\n",
      "   2. time_in_hospital\n",
      "   3. num_lab_procedures\n",
      "   4. num_medications\n",
      "   5. number_outpatient\n",
      "   6. number_emergency\n",
      "   7. number_inpatient\n",
      "   8. number_diagnoses\n",
      "   9. gender_Male\n",
      "  10. insulin_Yes\n",
      "  11. diabetesMed_Yes\n"
     ]
    }
   ],
   "source": [
    "# Save model artifacts\n",
    "model_dir = Path(\"../model\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = model_dir / \"ann_model.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"âœ… Model saved to: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = model_dir / \"scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ… Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save feature names (CRITICAL for prediction)\n",
    "feature_names_path = model_dir / \"feature_names.pkl\"\n",
    "joblib.dump(list(X_encoded.columns), feature_names_path)\n",
    "print(f\"âœ… Feature names saved to: {feature_names_path}\")\n",
    "print(f\"\\nðŸ“‹ Saved {len(X_encoded.columns)} feature names:\")\n",
    "for i, name in enumerate(list(X_encoded.columns), 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING PREDICTION PIPELINE\n",
      "============================================================\n",
      "\n",
      "ðŸ“¥ Input data:\n",
      "  age: 50\n",
      "  gender: Male\n",
      "  time_in_hospital: 5\n",
      "  num_lab_procedures: 45\n",
      "  num_medications: 15\n",
      "  number_outpatient: 2\n",
      "  number_emergency: 1\n",
      "  number_inpatient: 1\n",
      "  number_diagnoses: 5\n",
      "  insulin: Yes\n",
      "  diabetesMed: Yes\n",
      "\n",
      "âœ… Encoded features: 11 columns\n",
      "\n",
      "ðŸ“Š Feature values (non-zero):\n",
      "  age: 50\n",
      "  time_in_hospital: 5\n",
      "  num_lab_procedures: 45\n",
      "  num_medications: 15\n",
      "  number_outpatient: 2\n",
      "  number_emergency: 1\n",
      "  number_inpatient: 1\n",
      "  number_diagnoses: 5\n",
      "\n",
      "ðŸŽ¯ Prediction Results:\n",
      "  Risk Probability: 4.53%\n",
      "  Risk Level: Low Risk\n",
      "\n",
      "âœ… Prediction pipeline working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test prediction with sample data matching frontend format\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING PREDICTION PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample data matching what frontend sends\n",
    "test_sample = {\n",
    "    'age': 50,\n",
    "    'gender': 'Male',\n",
    "    'time_in_hospital': 5,\n",
    "    'num_lab_procedures': 45,\n",
    "    'num_medications': 15,\n",
    "    'number_outpatient': 2,\n",
    "    'number_emergency': 1,\n",
    "    'number_inpatient': 1,\n",
    "    'number_diagnoses': 5,\n",
    "    'insulin': 'Yes',\n",
    "    'diabetesMed': 'Yes'\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“¥ Input data:\")\n",
    "for key, value in test_sample.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame([test_sample])\n",
    "\n",
    "# One-hot encode (same as training)\n",
    "test_encoded = pd.get_dummies(test_df, drop_first=True)\n",
    "\n",
    "# Align with training features (fill missing with 0)\n",
    "test_aligned = test_encoded.reindex(columns=X_encoded.columns, fill_value=0)\n",
    "\n",
    "print(f\"\\nâœ… Encoded features: {test_aligned.shape[1]} columns\")\n",
    "print(f\"\\nðŸ“Š Feature values (non-zero):\")\n",
    "non_zero = test_aligned.loc[0, test_aligned.loc[0] != 0]\n",
    "for col, val in non_zero.items():\n",
    "    print(f\"  {col}: {val}\")\n",
    "\n",
    "# Scale\n",
    "test_scaled = scaler.transform(test_aligned)\n",
    "\n",
    "# Predict\n",
    "probability = model.predict_proba(test_scaled)[0][1]\n",
    "risk_level = model.predict(test_scaled)[0]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Prediction Results:\")\n",
    "print(f\"  Risk Probability: {probability*100:.2f}%\")\n",
    "print(f\"  Risk Level: {'High Risk' if risk_level == 1 else 'Low Risk'}\")\n",
    "print(f\"\\nâœ… Prediction pipeline working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DR Risk Predictor)",
   "language": "python",
   "name": "dr-risk-predictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
